name: inverse
layout: true
class: center, middle, inverse
---
##Introduction to
#Parallel Programming

.author[Stig Rune Jensen]

.date[22 November 2017, Narvik]

.footnote[Slides available on [GitHub](https://github.com/stigrj/parallel-tutorial)]

---

template: inverse
# Why parallel programming?

---

layout: false
.left-column[
## Why?
### Moore's law 
]
.right-column[
- Component doubling every 18 months
- Inflection point at ~3.5GHz reached early 2000's
- Continues to grow through multicore chips
- Cloud platforms and supercomputer clusters

<img src="images/moore.jpg" style="width: 100%; float: left">
]

---

template: inverse
# What is parallel programming?

---

layout: false
.left-column[
## What?
### Difficult
]
.right-column[
### Parallel programming is hard
- Difficult to obtain efficiency
- Difficult to verify correctness
- Difficult to debug
- Usually requires new algorithms

<img src="images/bugs.jpg" style="width: 70%; float: left">
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
### Amdahl's law of parallel efficiency
- Parallelizing 95% of the work load in your code yields
  __at most__ a 20-fold speedup

<img src="images/amdahl.jpg" style="width: 100%; float: left">
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
### Parallel programming involves
- Distributing work among CPUs
- Synchronization between CPUs
- Distributing memory among machines
- Communication between machines

]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
### Parallel programming involves
- Distributing work among CPUs
- Synchronization between CPUs
- Distributing memory among machines
- Communication between machines

### It is important to
- Balance the work load
- Limit memory duplication
- Limit communication
- Avoid bottlenecks

]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
<img src="images/parallel_efficiency_1.jpg" style="width: 100%; float: left">
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
<img src="images/parallel_efficiency_2.jpg" style="width: 100%; float: left">
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
<img src="images/parallel_efficiency_3.jpg" style="width: 100%; float: left">
]

---

template: inverse
# How to compute in parallel?

---

layout: false
.left-column[
## How?
### OpenMP
]
.right-column[
### Parallelization with OpenMP
- Shared memory (single machine)
- Work distribution through loops
- No data distribution
- No communication
- Thread based

<img src="images/parallel_omp.jpg" style="width: 60%; float: left">

]

---

layout: false
.left-column[
## How?
### OpenMP
]
.right-column[
### Pros
- Relatively simple implementation
- Quick way to good performance
- Simple load balancing
- No communication

]

---

layout: false
.left-column[
## How?
### OpenMP
]
.right-column[
### Pros
- Relatively simple implementation
- Quick way to good performance
- Simple load balancing
- No communication

### Cons
- Small scale parallelization (< ~50 CPUs)
- Limited memory
- Dead locks
- Race conditions
- Tedious debugging
]

---

layout: false
.left-column[
## How?
### OpenMP
### MPI
]
.right-column[
### Parallelization with Message Passing Interface (MPI)
- Distributed memory (multiple machines)
- Work distribution is __user specified__
- Data distributed is __user specified__
- Communication necessary
- Process based

### Master/Slave strategy

<img src="images/parallel_mpi_1.jpg" style="width: 60%; float: left">

]

---

layout: false
.left-column[
## How?
### OpenMP
### MPI
]
.right-column[
### Parallelization with Message Passing Interface (MPI)
- Distributed memory (multiple machines)
- Work distribution is __user specified__
- Data distributed is __user specified__
- Communication necessary
- Process based

### Domain (data) decomposition strategy

<img src="images/parallel_mpi_2.jpg" style="width: 60%; float: left">

]

---

layout: false
.left-column[
## How?
### OpenMP
### MPI
]
.right-column[
### Pros
- Large scale parallelization
- Extensive memory
]

---

layout: false
.left-column[
## How?
### OpenMP
### MPI
]
.right-column[
### Pros
- Large scale parallelization
- Extensive memory

### Cons
- Complicated implementation
- Often require complete rewrite
- Very much relies on the programmer
- Diffucult to load balance
- Communication overhead
]

---

template: inverse
# Example

---

layout: false
.left-column[
## Example
]
.right-column[
### Calculation of `\( \pi \)` through the integral
`$$ \frac{\pi}{4} = \int_0^1 \frac{1}{1+x^2} dx $$`

### Numerical approximation `\( f(x) = (1+x^2)^{-1} \)`
`$$ \frac{\pi}{4} \approx \sum_i^N f(x_i^\ast) \Delta x_i $$`

<img src="images/numerical_integral.jpg" style="width: 70%; float: left">

]

---

template: inverse
# How can we help?

---

layout: false
.left-column[
## Help?
### AUS
]
.right-column[

<img src="images/uninett_sigma2.jpg" style="width: 90%; float: left">

### [Uninett Sigma2](https://www.sigma2.no) offers [Advanced User Support](https://www.sigma2.no/content/advanced-user-support) on:
- Code parallelization
- Code optimization
- Code modularization
- Improving user interfaces
- Benchmarking
- Porting

]

---

name: last-page
template: inverse

Slideshow created using [remark] and served using [cicero]

[remark]: https://github.com/gnab/remark
[cicero]: https://github.com/bast/cicero
