name: inverse
layout: true
class: center, middle, inverse
---
##A crash course in
#Parallel Programming

.author[Stig Rune Jensen]

.date[22 November 2017, Narvik]

.footnote[Slides available on [GitHub](https://github.com/stigrj/parallel-programming-talk)]


---

template: inverse
# Why parallel programming?

---

layout: false
.left-column[
## Why?
### Moore's law 
]
.right-column[
- Component doubling every 18 months
- Inflection point at ~3.5GHz reached early 2000's
- Continues to grow through multicore chips
- Cloud platforms and supercomputer clusters

<img src="images/moore.jpg" style="width: 100%; float: left">
]

---

template: inverse
# What is parallel programming?

---

layout: false
.left-column[
## What?
### Difficult
]
.right-column[
### Parallel programming is hard
- Difficult to obtain efficiency
- Difficult to verify correctness
- Difficult to debug
- Usually requires new algorithms

<img src="images/bugs.jpg" style="width: 70%; float: left">
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
### Amdahl's law of parallel efficiency
- Parallelizing 95% of your code yields __at most__ 20-fold speedup

<img src="images/amdahl.jpg" style="width: 100%; float: left">
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
### Parallel programming involves
- Distributing work among CPUs
- Synchronization between CPUs
- Distributing memory among machines
- Communication between machines

]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
### Parallel programming involves
- Distributing work among CPUs
- Synchronization between CPUs
- Distributing memory among machines
- Communication between machines

### It is important to
- Balance the work load
- Limit memory duplication
- Limit communication
- Avoid bottlenecks

]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
<img src="images/parallel_efficiency_1.jpg" style="width: 100%; float: left">
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
<img src="images/parallel_efficiency_2.jpg" style="width: 100%; float: left">
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
<img src="images/parallel_efficiency_3.jpg" style="width: 100%; float: left">
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
### Scaling
]
.right-column[
### Parallel scaling
#### Sequential execution:
- One woman can make a baby in nine months
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
### Scaling
]
.right-column[
### Parallel scaling
#### Sequential execution:
- One woman can make a baby in nine months

#### Strong parallel scaling:
- Can nine woman make a baby in one month?
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
### Scaling
]
.right-column[
### Parallel scaling
#### Sequential execution:
- One woman can make a baby in nine months

#### Strong parallel scaling:
- Can nine woman make a baby in one month?

#### Weak parallel scaling:
- Nine woman can make nine babies in nine months
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
### Scaling
]
.right-column[
### Parallel scaling
#### Sequential execution:
- One woman can make a baby in nine months

#### Strong parallel scaling:
- Can nine woman make a baby in one month?

#### Weak parallel scaling:
- Nine woman can make nine babies in nine months

### Strong scaling is hard to achieve, but usually not necessary
]

---

template: inverse
# How to compute in parallel?

---

layout: false
.left-column[
## How?
### OpenMP
]
.right-column[
### Parallelization with OpenMP
- Shared memory (single machine)
- Work distribution through loops
- No data distribution
- Implicit synchronization
- No communication
- Thread based

<img src="images/parallel_omp.jpg" style="width: 60%; float: left">

]

---

layout: false
.left-column[
## How?
### OpenMP
]
.right-column[
### Pros
- Relatively simple implementation
- Quick way to good performance
- Simple load balancing
- No communication

]

---

layout: false
.left-column[
## How?
### OpenMP
]
.right-column[
### Pros
- Relatively simple implementation
- Quick way to good performance
- Simple load balancing
- No communication

### Cons
- Small scale parallelization (< ~50 CPUs)
- Limited memory
- Dead locks
- Race conditions
- Tedious debugging
]

---

layout: false
.left-column[
## How?
### OpenMP
### MPI
]
.right-column[
### Parallelization with Message Passing Interface (MPI)
- Distributed memory (multiple machines)
- Work distribution is __user specified__
- Data distributed is __user specified__
- Communication necessary
- Process based

### Master/Slave

<img src="images/parallel_mpi_1.jpg" style="width: 60%; float: left">

]

---

layout: false
.left-column[
## How?
### OpenMP
### MPI
]
.right-column[
### Parallelization with Message Passing Interface (MPI)
- Distributed memory (multiple machines)
- Work distribution is __user specified__
- Data distributed is __user specified__
- Communication necessary
- Process based

### Data decomposition

<img src="images/parallel_mpi_2.jpg" style="width: 60%; float: left">

]

---

layout: false
.left-column[
## How?
### OpenMP
### MPI
]
.right-column[
### Pros
- Large scale parallelization
- Extensive memory
]

---

layout: false
.left-column[
## How?
### OpenMP
### MPI
]
.right-column[
### Pros
- Large scale parallelization
- Extensive memory

### Cons
- Complicated implementation
- Often require complete rewrite
- Very much relies on the programmer
- Diffucult to load balance
- Communication overhead
]

---

template: inverse
# Example

---

layout: false
.left-column[
## Example
]
.right-column[
### Calculation of `\( \pi \)` using numerical integration
- The area of the unit circle is
    `$$ A = \pi r^2 = \pi $$`
]

---

layout: false
.left-column[
## Example
### Serial
### OpenMP
]
.right-column[
### OpenMP features

```c
#include "omp.h"                         // include OpenMP features

#pragma omp parallel                     // start parallel section
{

int my_id = omp_get_thread_num();        // get thread id
int n_threads = omp_get_num_threads();   // get number of threads
int max_threads = omp_get_max_threads(); // get max available threads

#pragma omp for                          // parallelize following loop
for (int i = 0; i < N; i++) {
    // hard work
}

}                                        // end parallel section
```
]

---

layout: false
.left-column[
## Example
### Serial
### OpenMP
]
.right-column[
### OpenMP variables

- __Shared__: all threads have read/write access to the same variable
- __Private__: all threads have a local copy of the variable
- Variables defined _inside_ the parallel section are always __private__
- Variables defined _outside_ can be passed to the parallel section using
  * `shared()`: variable is shared to all threads
  * `private()`: each thread gets an uninitialized local copy
  * `firstprivate()`: each thread gets a local copy, initialized to its outside value

```c
#include "omp.h"

int a = 1.0;
int b = 1.0;
int c = 1.0;
int d;

#pragma omp parallel shared(a), private(b), firstprivate(c)
{
int e = 1.0;

// all variables a-e are available in the parallel section:
// a is shared, value 1.0
// b is private, value uninitialized
// c is private, value 1.0
// d is shared, value uninitialized
// e is private, value 1.0
}
```
]

---

layout: false
.left-column[
## Example
### Serial
### OpenMP
]
.right-column[
### OpenMP work sharing

- `for` loops can be distributed among the threads in the following way:
  * `schedule(static)`: loop is divided in consecutive equally sized batches
  * `schedule(dynamic, batch)`: loop is distributed dynamically, with `batch` entries at the time
  * `schedule(guided)`: loop is distributed dynamically with smaller batch sizes towards the end
- `dynamic` and `guided` will be better load balanced, but has a higher overhead

```c
#include "omp.h"

int N = 10000; // total number of iterations

#pragma omp parallel firstprivate(N)
{

int n_iter = 0;
#pragma omp for schedule(guided)
for (int i = 0; i < N; i++) {
    n_iter++;
}
// n_iter will now be approximately (but not exactly) equal among all threads

}
```
]

---

layout: false
.left-column[
## Example
### Serial
### OpenMP
]
.right-column[
### OpenMP synchronization
- To avoid race conditions on shared variables, the following synchronizations
  are useful
  * `barrier`: all threads must wait until all have reached this point
  * `critical`: only one thread will execute the enclosed code at the time
  * `atomic`: only one thread will execute the following operation at the time

```c
#include "omp.h"

#pragma omp parallel        // start parallel section
{

// this part of the code is execuded in parallel

#pragma omp critical
{
// this part of the code only execuded by one thread at the time
}

#pragma omp barrier         // wait until all threads have reached this point

// this part of the code is execuded in parallel

#pragma omp atomic
// this single statement is only execuded by one thread at the time

}                           // end parallel section
```
]

---

layout: false
.left-column[
## Example
### Serial
### OpenMP
]
.right-column[
### OpenMP reduction
- The `reduction` clause performs a reduction operation on the variable
- A private copy is created and initialized for each thread. At the end
  of the parallel section, the reduction operation is applied to all
  private copies of the variable, and the final result is written to
  the global shared variable.

```c
#include "omp.h"

int a = 0;
int b = 1;

#pragma omp parallel reduction(+:a) reduction(*:b)
{

// a is here a private variable, initialized to 0
// b is here a private variable, initialized to 1

a += 1;
b *= 2;

// private variable are updated to
// a = 1
// b = 2

}

// now a and b have been collected from all threads
// a = (number of threads)
// b = 2^(number of threads)
```
]

---

layout: false
.left-column[
## Example
### Serial
### OpenMP
]
.right-column[
### Compiling and running the code
- Compiling using the GNU compiler
```bash
$ gcc -fopenmp source.c
```

- Running the code using 4 threads
```bash
$ OMP_NUM_THREADS=4 ./a.out
```

- Alternatively
```bash
$ export OMP_NUM_THREADS=4
$ ./a.out
```
]

---

layout: false
.left-column[
## Example
### Serial
### OpenMP
### MPI
]
.right-column[
MPI implementation
]

---

template: inverse
# How can we help?

---

layout: false
.left-column[
## Help?
### AUS
]
.right-column[

<img src="images/uninett_sigma2.jpg" style="width: 90%; float: left">

### [Uninett Sigma2](https://www.sigma2.no) offers [Advanced User Support](https://www.sigma2.no/content/advanced-user-support) on:
- Code parallelization
- Code optimization
- Code modularization
- Improving user interfaces
- Benchmarking
- Porting

]

---

name: last-page
template: inverse

Slideshow created using [remark] and served using [cicero]

[remark]: https://github.com/gnab/remark
[cicero]: https://github.com/bast/cicero
