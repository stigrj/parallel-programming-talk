name: inverse
layout: true
class: center, middle, inverse
---
##A crash course in
#Parallel Programming

.author[Stig Rune Jensen]

.date[22 November 2017, Narvik]

.footnote[Slides available on [GitHub](https://github.com/stigrj/parallel-programming-talk)]


---

template: inverse
# Why parallel programming?

---

layout: false
.left-column[
## Why?
### Moore's law 
]
.right-column[
- Component doubling every 18 months
- Inflection point at ~3.5GHz reached early 2000's
- Continues to grow through multicore chips
- Cloud platforms and supercomputer clusters

<img src="images/moore.jpg" style="width: 100%; float: left">
]

---

template: inverse
# What is parallel programming?

---

layout: false
.left-column[
## What?
### Difficult
]
.right-column[
### Parallel programming is hard
- Difficult to obtain efficiency
- Difficult to verify correctness
- Difficult to debug
- Usually requires new algorithms

<img src="images/bugs.jpg" style="width: 70%; float: left">
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
### Amdahl's law of parallel efficiency
- Parallelizing 95% of your code yields __at most__ 20-fold speedup

<img src="images/amdahl.jpg" style="width: 100%; float: left">
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
### Parallel programming involves
- Distributing work among CPUs
- Synchronization between CPUs
- Distributing memory among machines
- Communication between machines

]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
### Parallel programming involves
- Distributing work among CPUs
- Synchronization between CPUs
- Distributing memory among machines
- Communication between machines

### It is important to
- Balance the work load
- Limit memory duplication
- Limit communication
- Avoid bottlenecks

]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
<img src="images/parallel_efficiency_1.jpg" style="width: 100%; float: left">
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
<img src="images/parallel_efficiency_2.jpg" style="width: 100%; float: left">
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
]
.right-column[
<img src="images/parallel_efficiency_3.jpg" style="width: 100%; float: left">
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
### Scaling
]
.right-column[
### Parallel scaling
#### Sequential execution:
- One woman can make a baby in nine months
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
### Scaling
]
.right-column[
### Parallel scaling
#### Sequential execution:
- One woman can make a baby in nine months

#### Strong parallel scaling:
- Can nine woman make a baby in one month?
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
### Scaling
]
.right-column[
### Parallel scaling
#### Sequential execution:
- One woman can make a baby in nine months

#### Strong parallel scaling:
- Can nine woman make a baby in one month?

#### Weak parallel scaling:
- Nine woman can make nine babies in nine months
]

---

layout: false
.left-column[
## What?
### Difficult
### Efficiency
### Scaling
]
.right-column[
### Parallel scaling
#### Sequential execution:
- One woman can make a baby in nine months

#### Strong parallel scaling:
- Can nine woman make a baby in one month?

#### Weak parallel scaling:
- Nine woman can make nine babies in nine months

### Strong scaling is hard to achieve, but usually not necessary
]

---

template: inverse
# How to compute in parallel?

---

layout: false
.left-column[
## How?
### OpenMP
]
.right-column[
### Parallelization with OpenMP
- Shared memory (single machine)
- Work distribution through loops
- No data distribution
- Implicit synchronization
- No communication
- Thread based

<img src="images/parallel_omp.jpg" style="width: 60%; float: left">

]

---

layout: false
.left-column[
## How?
### OpenMP
]
.right-column[
### Pros
- Relatively simple implementation
- Quick way to good performance
- Simple load balancing
- No communication

]

---

layout: false
.left-column[
## How?
### OpenMP
]
.right-column[
### Pros
- Relatively simple implementation
- Quick way to good performance
- Simple load balancing
- No communication

### Cons
- Small scale parallelization (< ~50 CPUs)
- Limited memory
- Dead locks
- Race conditions
- Tedious debugging
]

---

layout: false
.left-column[
## How?
### OpenMP
### MPI
]
.right-column[
### Parallelization with Message Passing Interface (MPI)
- Distributed memory (multiple machines)
- Work distribution is __user specified__
- Data distributed is __user specified__
- Communication necessary
- Process based

### Master/Slave strategy

<img src="images/parallel_mpi_1.jpg" style="width: 60%; float: left">

]

---

layout: false
.left-column[
## How?
### OpenMP
### MPI
]
.right-column[
### Parallelization with Message Passing Interface (MPI)
- Distributed memory (multiple machines)
- Work distribution is __user specified__
- Data distributed is __user specified__
- Communication necessary
- Process based

### Data decomposition strategy

<img src="images/parallel_mpi_2.jpg" style="width: 60%; float: left">

]

---

layout: false
.left-column[
## How?
### OpenMP
### MPI
]
.right-column[
### Pros
- Large scale parallelization
- Extensive memory
]

---

layout: false
.left-column[
## How?
### OpenMP
### MPI
]
.right-column[
### Pros
- Large scale parallelization
- Extensive memory

### Cons
- Complicated implementation
- Often require complete rewrite
- Very much relies on the programmer
- Diffucult to load balance
- Communication overhead
]

---

template: inverse
# Example

---

layout: false
.left-column[
## Example
]
.right-column[
### Calculation of `\( \pi \)` using numerical integration
`$$ \int_0^1 \frac{1}{1+x^2} = \frac{\pi}{4} $$`

<img src="images/numerical_integral.jpg" style="width: 80%; float: left">

]

---

template: inverse
# Syntax

---

layout: false
.left-column[
## Syntax
### OpenMP
]
.right-column[
### OpenMP parallel section
- OpenMP features are added using `#pragma omp`
- `#pragma omp parallel num_threads(N)` will spawn `N`
  threads that will execute the enclosed code in parallel
- By default `num_threads` will be the maximum available
]

---

layout: false
.left-column[
## Syntax
### OpenMP
]
.right-column[
### OpenMP parallel section
```c
#include "omp.h"

#pragma omp parallel
{ // start parallel section

// get my thread number
int my_id = omp_get_thread_num();

// get number of threads in this section
int n_threads = omp_get_num_threads();

// get maximum number of threads
int max_threads = omp_get_max_threads();

} // end parallel section
```
]

---

layout: false
.left-column[
## Syntax
### OpenMP
]
.right-column[
### OpenMP variables
- __shared__: all threads have access to the variable
- __private__: all threads have a local copy of the variable
- A variable defined _inside_ a parallel section is private
- A variables defined _outside_ can be passed to the parallel section using
  * `shared()`: variable is shared between threads, this is the default
  * `private()`: each thread gets an uninitialized copy of the variable
  * `firstprivate()`: each thread gets a local copy, 
    initialized to its outside value
]

---

layout: false
.left-column[
## Syntax
### OpenMP
]
.right-column[
### OpenMP variables
```c
double a = 1.0;
double b = 1.0;
double c = 1.0;
double d;

#pragma omp parallel shared(a) private(b) firstprivate(c)
{ // start parallel section

double e = 1.0;

// all variables a-e are available in the parallel section:
// a is shared, value 1.0
// b is private, value uninitialized
// c is private, value 1.0
// d is shared, value uninitialized
// e is private, value 1.0

} // end parallel section
```
]

---

layout: false
.left-column[
## Syntax
### OpenMP
]
.right-column[
### OpenMP work sharing
- `for` loops can be distributed among the threads in the following way:
  * `schedule(static)`: loop is divided in consecutive equally sized batches,
    this is the default
  * `schedule(dynamic, batch)`: loop is distributed dynamically, with `batch`
    entries at the time
  * `schedule(guided)`: loop is distributed dynamically with smaller batches
    toward the end
- `dynamic` and `guided` will be better load balanced, but have larger overhead
]

---

layout: false
.left-column[
## Syntax
### OpenMP
]
.right-column[
### OpenMP work sharing
```c
int N = 10000; // total number of iterations

#pragma omp parallel firstprivate(N)
{ // start parallel section

int n_iter = 0;

#pragma omp for schedule(guided)
for (int i = 0; i < N; i++) {
    n_iter++;
}
// n_iter will now be approximately (but not exactly)
// equal among all threads

} // end parallel section
```
]

---

layout: false
.left-column[
## Syntax
### OpenMP
]
.right-column[
### OpenMP synchronization
- To avoid race conditions on shared variables, the following synchronizations
  are useful
  * `barrier`: __all__ threads must wait until __all__ threads have reached this point
  * `critical`: __all__ threads will execute the following code, but only __one__ at the time
  * `master`: only __master__ thread will execute the following code
  * `single`: only __one__ (random) thread will execute the following code
  * `atomic`: __all__ threads will execute the following __single operation__, but only __one__ at the time
]

---

layout: false
.left-column[
## Syntax
### OpenMP
]
.right-column[
### OpenMP synchronization
```c
#pragma omp parallel
{ // start parallel section

// this part of the code is execuded in parallel

#pragma omp critical
{
// this part of the code will be executed by all
// threads, one at the time
}

// wait until all threads have reached this point
#pragma omp barrier

// this part of the code is execuded in parallel

#pragma omp single
{
// this part of the code will be executed by one thread
}

} // end parallel section
```
]

---

layout: false
.left-column[
## Syntax
### OpenMP
]
.right-column[
### OpenMP reduction
- The `reduction` clause performs a reduction operation on the variable
- A private copy is created and initialized for each thread. At the end
  of the parallel section, the reduction operation is applied to all
  private copies of the variable, and the final result is written to
  the global shared variable.
]

---

layout: false
.left-column[
## Syntax
### OpenMP
]
.right-column[
```c
int a = 0;
int b = 1;

#pragma omp parallel reduction(+:a) reduction(*:b)
{ // start parallel section

// a is here a private variable, initialized to 0
// b is here a private variable, initialized to 1

a += 1;
b *= 2;

// private variable are updated to
// a = 1
// b = 2

} // end parallel section

// now a and b have been collected from all threads
// a = (number of threads)
// b = 2^(number of threads)
```
]

---

layout: false
.left-column[
## Syntax
### OpenMP
]
.right-column[
### Compiling and running the code
- Compiling using the GNU compiler
```bash
$ gcc -fopenmp source.c
```

- With optimization
```bash
$ gcc -O -fopenmp source.c
```

- Running the code using 4 threads
```bash
$ OMP_NUM_THREADS=4 ./a.out
```

- Alternatively
```bash
$ export OMP_NUM_THREADS=4
$ ./a.out
```
]

---

layout: false
.left-column[
## Syntax
### OpenMP
### MPI
]
.right-column[
MPI implementation
]

---

template: inverse
# How can we help?

---

layout: false
.left-column[
## Help?
### AUS
]
.right-column[

<img src="images/uninett_sigma2.jpg" style="width: 90%; float: left">

### [Uninett Sigma2](https://www.sigma2.no) offers [Advanced User Support](https://www.sigma2.no/content/advanced-user-support) on:
- Code parallelization
- Code optimization
- Code modularization
- Improving user interfaces
- Benchmarking
- Porting

]

---

name: last-page
template: inverse

Slideshow created using [remark] and served using [cicero]

[remark]: https://github.com/gnab/remark
[cicero]: https://github.com/bast/cicero
